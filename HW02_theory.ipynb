{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71e5a95",
   "metadata": {},
   "source": [
    "# Home Assignment No. 2: Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9681ded3",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "\n",
    "* For every separate problem you can get **INTERMEDIATE scores**.\n",
    "\n",
    "\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "\n",
    "\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but *discouraged*.\n",
    "\n",
    "## $\\LaTeX$ in Jupyter\n",
    "\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "\n",
    "* to write **cases of equations** use \n",
    "```markdown\n",
    "$$ left-hand-side = \\begin{cases}\n",
    "                     right-hand-side on line 1, & \\text{condition} \\\\\n",
    "                     right-hand-side on line 2, & \\text{condition} \\\\\n",
    "                    \\end{cases} $$\n",
    "```\n",
    "\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "$$ \\begin{align}\n",
    "    left-hand-side on line 1 &= right-hand-side on line 1 \\\\\n",
    "    left-hand-side on line 2 &= right-hand-side on line 2\n",
    "   \\end{align} $$\n",
    "```\n",
    "\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61cee62",
   "metadata": {},
   "source": [
    "## Task 1: Kernel theory [8 points]\n",
    "\n",
    "Let $K(x, x'): \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}$ be a kernel, and $\\phi: \\mathbb{R}^n \\to \\mathbb{R}^m$ its **unknown** feature mapping. For $x, x' \\in \\mathbb{R}^n$ derive the squared Euclidean distance between $\\phi(x)$ and $\\phi(x')$ only in terms of $K(\\cdot, \\cdot)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e07a2f",
   "metadata": {},
   "source": [
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15374dff",
   "metadata": {},
   "source": [
    "Euclidean distance: $$ D^2 = \\|\\phi(x) - \\phi(x')\\|^2 $$ \n",
    "expand to: $$ D^2 = (\\phi(x) - \\phi(x')) \\cdot (\\phi(x) - \\phi(x')) $$\n",
    "\n",
    "rewrite: $$ D^2 = \\phi(x) \\cdot \\phi(x) - 2 \\phi(x) \\cdot \\phi(x') + \\phi(x') \\cdot \\phi(x') $$\n",
    "\n",
    "substitute to express terms of kernel function: $$ D^2 = K(x, x) - 2K(x, x') + K(x', x') $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f6720",
   "metadata": {},
   "source": [
    "## Task 2: SVM [9 points]\n",
    "\n",
    "Show that for a two-class SVM classifier trained on a linearly separable dataset $(x_i, y_i)_{i =1}^n$ the following upper bound on the leave-one-out-cross-validation error holds true:\n",
    "\n",
    "$$\n",
    "L_1OCV = \\frac{1}{n} \\sum_{i = 1}^n \\delta(y_i \\ne f_i(x_i)) \\le \\frac{|SV|}{n},\n",
    "$$\n",
    "\n",
    "where $\\delta(c) = 1$ if $c$ is True and $\\delta(c) = 0$ if $c$ is False;  \n",
    "for all $i = 1, \\dots, n$ $f_i(x_i)$ is the SVM classifier fitted on the entire data without the observation $(x_i, y_i)$ and $|SV|$ is the number of support vectors of the SVM classifier fitted on the entire data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb3f490",
   "metadata": {},
   "source": [
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ff523e",
   "metadata": {},
   "source": [
    "Definition of the LOOCV Error for SVM:\n",
    "(leave-one-out-cross-validation error:  fraction of times SVM classifier, trained on all data except one observation, misclassifies that left-out observation)\n",
    "\n",
    "$$\n",
    "L_{1OCV} = \\frac{1}{n} \\sum_{i = 1}^n \\delta(y_i \\ne f_i(x_i))\n",
    "$$\n",
    "\n",
    "upper bound number of support vectors (SV):\n",
    "(shows  that this error is bounded above by fraction of support vectors in the SVM classifier fitted on  dataset)\n",
    "$$\n",
    "L_{1OCV} \\le \\frac{|SV|}{n}\n",
    "$$\n",
    "\n",
    "The reason for this is that each missclassification in the LOOCV in a linearly seperable case implies that the left-out point was a SV in the original SVM model, because for linearly seperable data, the decision boundry is entirely determined by the SVßs. Taking away/excluding a non-support vector doesnßt alter decision boundry - a misclassification can only occur if a SV is removed. In worst-case all SVßs removed -> leads to a misclassifciation - number of missclassifications can however not exceed numbr of support vectors. \n",
    "-> therefore holds true that LOOCV error is bounded by ratio of number of SVs (|SV|) to total numbr. of observations (n). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb203f",
   "metadata": {},
   "source": [
    "## Task 3. Decision Tree Leaves [6 points]\n",
    "\n",
    "Consider a leaf of a decision tree that consists of object-label pairs $(x_{1}, y_{1}), \\dots, (x_{n}, y_{n})$.\n",
    "\n",
    "The prediction $\\hat{y}$ of this leaf is defined to minimize the loss on the training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdefc354",
   "metadata": {},
   "source": [
    "Find the **optimal prediction** in the leaf, for a regression tree, i.e. $y_{i} \\in \\mathbb{R}$, and squared percentage error loss $\\mathcal{L}(y, \\hat{y}) = \\cfrac{\\left(y - \\hat{y} \\right)^{2}}{y^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa5b6e",
   "metadata": {},
   "source": [
    "### Your solution:\n",
    "\n",
    " Desion tree leaf with object-label pairs $(x_{1}, y_{1}), \\dots, (x_{n}, y_{n})$, where $y_{i} \\in \\mathbb{R}$, Want to find prediciton y_hat $\\hat{y}$ which minimizes squared percetage error loss\n",
    "$$\n",
    "\\mathcal{L}(y, \\hat{y}) = \\frac{(y - \\hat{y})^{2}}{y^2}\n",
    "$$\n",
    "\n",
    "Goal minimize average loss across all paris in leaf: \n",
    "$$\n",
    "\\bar{\\mathcal{L}}(\\hat{y}) = \\frac{1}{n} \\sum_{i = 1}^{n} \\frac{(y_i - \\hat{y})^2}{y_i^2}\n",
    "$$\n",
    "\n",
    "Then find optimal y-hat $\\hat{y}$ by differentiating $\\bar{\\mathcal{L}}(\\hat{y}) $ with respect to y-hat and set it to zero: \n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\hat{y}} \\bar{\\mathcal{L}}(\\hat{y}) = 0   \n",
    "$$\n",
    "so: \n",
    "$$     \n",
    "\\frac{1}{n}  \\sum_{i = 1}^{n} ( -2 \\frac{(y_i - \\hat{y})^2}{y_i^2} ) = 0 \n",
    "$$\n",
    "\n",
    "this could be compuationally complex - depends pm yi value in dataset - numerical methods like root-finding mgiht me helpful in solfing the derivative "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
